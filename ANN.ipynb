{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "# from nltk import bigrams,trigrams\n",
    "from sklearn.feature_extraction.text import CountVectorizer,TfidfVectorizer\n",
    "from sklearn.ensemble import GradientBoostingClassifier, RandomForestClassifier\n",
    "import re\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score,precision_score,recall_score,f1_score,classification_report\n",
    "from textblob import TextBlob\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from nltk.corpus import stopwords\n",
    "import nltk\n",
    "from nltk.stem import PorterStemmer\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "import eli5\n",
    "from eli5.sklearn import PermutationImportance\n",
    "from sklearn.svm import SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import datasets # https://www.tensorflow.org/api_docs/python/tf/keras/datasets\n",
    "from tensorflow.keras import layers # https://www.tensorflow.org/api_docs/python/tf/keras/layers\n",
    "from tensorflow.keras import activations # https://www.tensorflow.org/api_docs/python/tf/keras/activations\n",
    "from tensorflow.keras import initializers # https://www.tensorflow.org/api_docs/python/tf/keras/initializers\n",
    "from tensorflow.keras import losses # https://www.tensorflow.org/api_docs/python/tf/keras/losses\n",
    "from tensorflow.keras import metrics # https://www.tensorflow.org/api_docs/python/tf/keras/metrics\n",
    "from tensorflow.keras import optimizers # https://www.tensorflow.org/api_docs/python/tf/keras/optimizers\n",
    "from tensorflow.keras import regularizers # https://www.tensorflow.org/api_docs/python/tf/keras/regularizers\n",
    "from tensorflow.keras.optimizers import schedules # https://www.tensorflow.org/api_docs/python/tf/keras/optimizers/schedules\n",
    "from tensorflow.keras import callbacks # https://www.tensorflow.org/api_docs/python/tf/keras/callbacks\n",
    "from tensorflow.keras import utils # https://www.tensorflow.org/api_docs/python/tf/keras/utils\n",
    "from tensorflow.keras import models # https://www.tensorflow.org/api_docs/python/tf/keras/models\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>polarity</th>\n",
       "      <th>subjectivity</th>\n",
       "      <th>wordCount</th>\n",
       "      <th>stdrating</th>\n",
       "      <th>clusterid</th>\n",
       "      <th>meanSimilarity</th>\n",
       "      <th>varOfSimilarity</th>\n",
       "      <th>meanRating</th>\n",
       "      <th>numberOfReviews</th>\n",
       "      <th>avgNoOfWords</th>\n",
       "      <th>totalReviews</th>\n",
       "      <th>isSingleton</th>\n",
       "      <th>posReviewRatio</th>\n",
       "      <th>negReviewRatio</th>\n",
       "      <th>avgNoOfReviews</th>\n",
       "      <th>maxNoOfReviews</th>\n",
       "      <th>dateVariance</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.195833</td>\n",
       "      <td>0.395833</td>\n",
       "      <td>143</td>\n",
       "      <td>0.896514</td>\n",
       "      <td>141</td>\n",
       "      <td>0.043510</td>\n",
       "      <td>0.005451</td>\n",
       "      <td>4.009524</td>\n",
       "      <td>210</td>\n",
       "      <td>37.410256</td>\n",
       "      <td>39</td>\n",
       "      <td>0</td>\n",
       "      <td>0.974359</td>\n",
       "      <td>0.025641</td>\n",
       "      <td>1.625</td>\n",
       "      <td>3</td>\n",
       "      <td>1.360395e+02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.025000</td>\n",
       "      <td>0.650000</td>\n",
       "      <td>163</td>\n",
       "      <td>0.896514</td>\n",
       "      <td>74</td>\n",
       "      <td>0.067578</td>\n",
       "      <td>0.005659</td>\n",
       "      <td>4.009524</td>\n",
       "      <td>210</td>\n",
       "      <td>27.000000</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1</td>\n",
       "      <td>2.003740e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.220000</td>\n",
       "      <td>0.328718</td>\n",
       "      <td>115</td>\n",
       "      <td>0.896514</td>\n",
       "      <td>167</td>\n",
       "      <td>0.047143</td>\n",
       "      <td>0.005273</td>\n",
       "      <td>4.009524</td>\n",
       "      <td>210</td>\n",
       "      <td>22.000000</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.000</td>\n",
       "      <td>2</td>\n",
       "      <td>0.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.555134</td>\n",
       "      <td>0.776786</td>\n",
       "      <td>315</td>\n",
       "      <td>0.896514</td>\n",
       "      <td>378</td>\n",
       "      <td>0.076343</td>\n",
       "      <td>0.005720</td>\n",
       "      <td>4.009524</td>\n",
       "      <td>210</td>\n",
       "      <td>52.000000</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1</td>\n",
       "      <td>2.003740e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.138715</td>\n",
       "      <td>0.538294</td>\n",
       "      <td>420</td>\n",
       "      <td>0.896514</td>\n",
       "      <td>992</td>\n",
       "      <td>0.062976</td>\n",
       "      <td>0.005454</td>\n",
       "      <td>4.009524</td>\n",
       "      <td>210</td>\n",
       "      <td>86.200000</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1</td>\n",
       "      <td>2.618750e+01</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   polarity  subjectivity  wordCount  stdrating  clusterid  meanSimilarity  \\\n",
       "0  0.195833      0.395833        143   0.896514        141        0.043510   \n",
       "1  0.025000      0.650000        163   0.896514         74        0.067578   \n",
       "2  0.220000      0.328718        115   0.896514        167        0.047143   \n",
       "3  0.555134      0.776786        315   0.896514        378        0.076343   \n",
       "4  0.138715      0.538294        420   0.896514        992        0.062976   \n",
       "\n",
       "   varOfSimilarity  meanRating  numberOfReviews  avgNoOfWords  totalReviews  \\\n",
       "0         0.005451    4.009524              210     37.410256            39   \n",
       "1         0.005659    4.009524              210     27.000000             1   \n",
       "2         0.005273    4.009524              210     22.000000             2   \n",
       "3         0.005720    4.009524              210     52.000000             1   \n",
       "4         0.005454    4.009524              210     86.200000             5   \n",
       "\n",
       "   isSingleton  posReviewRatio  negReviewRatio  avgNoOfReviews  \\\n",
       "0            0        0.974359        0.025641           1.625   \n",
       "1            1        1.000000        0.000000           1.000   \n",
       "2            0        1.000000        0.000000           2.000   \n",
       "3            1        1.000000        0.000000           1.000   \n",
       "4            0        0.800000        0.200000           1.000   \n",
       "\n",
       "   maxNoOfReviews  dateVariance  \n",
       "0               3  1.360395e+02  \n",
       "1               1  2.003740e+06  \n",
       "2               2  0.000000e+00  \n",
       "3               1  2.003740e+06  \n",
       "4               1  2.618750e+01  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#reading all the 17 features\n",
    "all_features = pd.read_csv(\"featuresAll.csv\")\n",
    "labels = pd.read_csv(\"labels.csv\")\n",
    "all_features.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_features['labels'] = labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "only_one = all_features.loc[all_features['labels'] == 1]\n",
    "only_zero = all_features.loc[all_features['labels'] == 0]\n",
    "train_one = only_one.iloc[:29508]\n",
    "test_one = only_one.iloc[29508:]\n",
    "train_zero = only_zero.iloc[:257733]\n",
    "test_zero = only_zero.iloc[257733:]\n",
    "train = train_one.append(train_zero, ignore_index = True)\n",
    "test = test_one.append(test_zero, ignore_index = True)\n",
    "train = train.sample(frac=1).reset_index(drop=True)\n",
    "X_train = train.iloc[:,:17]\n",
    "Y_train = train.iloc[:,17]\n",
    "X_test = test.iloc[:,:17]\n",
    "Y_test = test.iloc[:,17]\n",
    "X_train = X_train.to_numpy()\n",
    "Y_train = Y_train.to_numpy()\n",
    "X_test = X_test.to_numpy()\n",
    "Y_test = Y_test.to_numpy()\n",
    "# np.random.shuffle(train)\n",
    "# np.random.shuffle(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(359052, 17)\n"
     ]
    }
   ],
   "source": [
    "# all_features = all_features.to_numpy()\n",
    "# print(all_features.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "359052\n"
     ]
    }
   ],
   "source": [
    "# #reading the reviews\n",
    "# file1 = open(\"YelpNYC/reviewContent\")\n",
    "# all_stop_words = stopwords.words('english')\n",
    "# data1 = []\n",
    "# for s in file1:\n",
    "#     row = s[17:]\n",
    "#     row = re.sub('[^a-zA-Z0-9 \\n\\..]', '', row)\n",
    "#     row = ' '.join([word for word in row.split() if word not in all_stop_words])\n",
    "#     data1.append(row)\n",
    "# file1.close()\n",
    "# print(len(data1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(359052, 1)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# #reading all the labels\n",
    "# labels = pd.read_csv(\"labels.csv\")\n",
    "# Y = labels.to_numpy()\n",
    "# Y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TF-IDF Vectorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #using tfidf vectorization to convert the reviews into features\n",
    "# tfidf_vector = TfidfVectorizer(stop_words='english')\n",
    "# X_tfidf = tfidf_vector.fit_transform(data1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #reducing the number of features\n",
    "# svd = TruncatedSVD(n_components = 100)\n",
    "# X_tfidf = svd.fit_transform(X_tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(359052, 17)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# #putting all the features together\n",
    "# # X1 = np.hstack((X_tfidf,all_features))\n",
    "# X1 = all_features\n",
    "# X1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "flatten (Flatten)            (None, 17)                0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 64)                1152      \n",
      "_________________________________________________________________\n",
      "batch_normalization (BatchNo (None, 64)                256       \n",
      "_________________________________________________________________\n",
      "activation (Activation)      (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 128)               8320      \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 128)               512       \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 128)               16512     \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 128)               512       \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 32)                4128      \n",
      "_________________________________________________________________\n",
      "batch_normalization_3 (Batch (None, 32)                128       \n",
      "_________________________________________________________________\n",
      "activation_3 (Activation)    (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 1)                 33        \n",
      "=================================================================\n",
      "Total params: 31,553\n",
      "Trainable params: 30,849\n",
      "Non-trainable params: 704\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = tf.keras.Sequential([\n",
    "\n",
    "    \n",
    "    layers.Flatten(input_dim = len(X_train[0,:])), \n",
    "    \n",
    "    layers.Dense(units=64, activation=None), \n",
    "    layers.BatchNormalization(),\n",
    "    layers.Activation(activations.relu),\n",
    "    \n",
    "    layers.Dense(units=128, activation=None), \n",
    "    layers.BatchNormalization(),\n",
    "    layers.Activation(activations.relu),\n",
    "    \n",
    "    layers.Dense(units=128, activation=None), \n",
    "    layers.BatchNormalization(),\n",
    "    layers.Activation(activations.relu),\n",
    "    \n",
    "    layers.Dense(units=32, activation=None), \n",
    "    layers.BatchNormalization(),\n",
    "    layers.Activation(activations.relu),\n",
    "    \n",
    "    layers.Dense(units=1, activation=activations.sigmoid) \n",
    "])\n",
    "\n",
    "model.summary()\n",
    "\n",
    "model.compile(\n",
    "    optimizer=optimizers.Adam(learning_rate=0.001), \n",
    "    loss=losses.BinaryCrossentropy(), \n",
    "    metrics=['accuracy']\n",
    ") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='binary_crossentropy',optimizer='adam',metrics='accuracy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "2245/2245 [==============================] - 10s 3ms/step - loss: 0.9595 - accuracy: 0.7292\n",
      "Epoch 2/10\n",
      "2245/2245 [==============================] - 7s 3ms/step - loss: 0.9838 - accuracy: 0.7339\n",
      "Epoch 3/10\n",
      "2245/2245 [==============================] - 7s 3ms/step - loss: 1.0018 - accuracy: 0.7348\n",
      "Epoch 4/10\n",
      "2245/2245 [==============================] - 7s 3ms/step - loss: 0.9897 - accuracy: 0.7330\n",
      "Epoch 5/10\n",
      "2245/2245 [==============================] - 7s 3ms/step - loss: 0.9858 - accuracy: 0.7365\n",
      "Epoch 6/10\n",
      "2245/2245 [==============================] - 7s 3ms/step - loss: 0.9973 - accuracy: 0.7331\n",
      "Epoch 7/10\n",
      "2245/2245 [==============================] - 7s 3ms/step - loss: 0.9883 - accuracy: 0.7344\n",
      "Epoch 8/10\n",
      "2245/2245 [==============================] - 7s 3ms/step - loss: 0.9915 - accuracy: 0.7350\n",
      "Epoch 9/10\n",
      "2245/2245 [==============================] - 7s 3ms/step - loss: 0.9806 - accuracy: 0.7359\n",
      "Epoch 10/10\n",
      "2245/2245 [==============================] - 8s 3ms/step - loss: 0.9879 - accuracy: 0.7343\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7fcbaee83b20>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(x=X_train, y=Y_train, epochs=10,batch_size=128, verbose=1,class_weight={0:1,1:7})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(71811,)\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      0.74      0.83     64434\n",
      "           1       0.23      0.66      0.34      7377\n",
      "\n",
      "    accuracy                           0.73     71811\n",
      "   macro avg       0.59      0.70      0.58     71811\n",
      "weighted avg       0.88      0.73      0.78     71811\n",
      "\n"
     ]
    }
   ],
   "source": [
    "predictions = np.round(model.predict(X_test))\n",
    "\n",
    "\n",
    "print(Y_test.shape)\n",
    "print(classification_report(Y_test,predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.ndarray"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# type(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for fake reviews in ANN:  0.6550088111698522\n",
      "Accuracy for true reviews in ANN:  0.7432256262221808\n"
     ]
    }
   ],
   "source": [
    "real_one = 0\n",
    "real_zero = 0\n",
    "pred_one = 0\n",
    "pred_zero = 0\n",
    "for i in range(len(predictions)):\n",
    "    if Y_test[i] == 1:\n",
    "        real_one += 1\n",
    "        if predictions[i] == 1:\n",
    "            pred_one += 1\n",
    "    else:\n",
    "        real_zero += 1\n",
    "        if predictions[i] == 0:\n",
    "            pred_zero += 1\n",
    "print(\"Accuracy for fake reviews in ANN: \",pred_one/real_one)\n",
    "print(\"Accuracy for true reviews in ANN: \",pred_zero/real_zero)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# perm = PermutationImportance(model).fit(X_train, Y_train)\n",
    "# eli5.show_weights(perm, feature_names = X_train.columns.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# svc = SVC(kernel='poly')\n",
    "# svc.fit(X_train,Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "flatten (Flatten)            (None, 17)                0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 64)                1152      \n",
      "_________________________________________________________________\n",
      "batch_normalization (BatchNo (None, 64)                256       \n",
      "_________________________________________________________________\n",
      "activation (Activation)      (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 128)               8320      \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 128)               512       \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 128)               16512     \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 128)               512       \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 32)                4128      \n",
      "_________________________________________________________________\n",
      "batch_normalization_3 (Batch (None, 32)                128       \n",
      "_________________________________________________________________\n",
      "activation_3 (Activation)    (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 1)                 33        \n",
      "=================================================================\n",
      "Total params: 31,553\n",
      "Trainable params: 30,849\n",
      "Non-trainable params: 704\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# models1 = tf.keras.Sequential([\n",
    "\n",
    "    \n",
    "#     layers.Flatten(input_dim = len(X1[0,:])), \n",
    "    \n",
    "#     layers.Dense(units=64, activation=None), \n",
    "#     layers.BatchNormalization(),\n",
    "#     layers.Activation(activations.relu),\n",
    "    \n",
    "#     layers.Dense(units=128, activation=None), \n",
    "#     layers.BatchNormalization(),\n",
    "#     layers.Activation(activations.relu),\n",
    "    \n",
    "#     layers.Dense(units=128, activation=None), \n",
    "#     layers.BatchNormalization(),\n",
    "#     layers.Activation(activations.relu),\n",
    "    \n",
    "#     layers.Dense(units=32, activation=None), \n",
    "#     layers.BatchNormalization(),\n",
    "#     layers.Activation(activations.relu),\n",
    "    \n",
    "#     layers.Dense(units=1, activation=activations.sigmoid) \n",
    "# ])\n",
    "\n",
    "# models1.summary()\n",
    "\n",
    "# models1.compile(\n",
    "#     optimizer=optimizers.Adam(learning_rate=0.001), \n",
    "#     loss=losses.BinaryCrossentropy(), \n",
    "#     metrics=['accuracy']\n",
    "# ) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "2245/2245 [==============================] - 5s 2ms/step - loss: 680.1175 - accuracy: 0.8246\n",
      "Epoch 2/5\n",
      "2245/2245 [==============================] - 4s 2ms/step - loss: 3.6577 - accuracy: 0.8796\n",
      "Epoch 3/5\n",
      "2245/2245 [==============================] - 5s 2ms/step - loss: 0.7007 - accuracy: 0.8971\n",
      "Epoch 4/5\n",
      "2245/2245 [==============================] - 4s 2ms/step - loss: 1.0891 - accuracy: 0.8973\n",
      "Epoch 5/5\n",
      "2245/2245 [==============================] - 4s 2ms/step - loss: 1.2254 - accuracy: 0.8973\n",
      "Fold 1 Accuracy: 0.8962693737728203  Precision:  0.3125  Recall:  0.008133387555917039  F1-Score:  0.015854141894569955\n",
      "Epoch 1/5\n",
      "2245/2245 [==============================] - 4s 2ms/step - loss: 1.3161 - accuracy: 0.8964\n",
      "Epoch 2/5\n",
      "2245/2245 [==============================] - 4s 2ms/step - loss: 1.4950 - accuracy: 0.8965\n",
      "Epoch 3/5\n",
      "2245/2245 [==============================] - 4s 2ms/step - loss: 1.0375 - accuracy: 0.8966\n",
      "Epoch 4/5\n",
      "2245/2245 [==============================] - 5s 2ms/step - loss: 0.6325 - accuracy: 0.8966\n",
      "Epoch 5/5\n",
      "2245/2245 [==============================] - 4s 2ms/step - loss: 0.8530 - accuracy: 0.8966\n",
      "Fold 2 Accuracy: 0.8972859311247581  Precision:  1.0  Recall:  0.00013555645926528398  F1-Score:  0.0002710761724044456\n",
      "Epoch 1/5\n",
      "2245/2245 [==============================] - 5s 2ms/step - loss: 0.4157 - accuracy: 0.8969\n",
      "Epoch 2/5\n",
      "2245/2245 [==============================] - 3s 2ms/step - loss: 0.5350 - accuracy: 0.8966\n",
      "Epoch 3/5\n",
      "2245/2245 [==============================] - 4s 2ms/step - loss: 0.4315 - accuracy: 0.8967\n",
      "Epoch 4/5\n",
      "2245/2245 [==============================] - 3s 2ms/step - loss: 0.4854 - accuracy: 0.8967\n",
      "Epoch 5/5\n",
      "2245/2245 [==============================] - 3s 2ms/step - loss: 0.6389 - accuracy: 0.8961\n",
      "Fold 3 Accuracy: 0.8972705751288121  Precision:  0.0  Recall:  0.0  F1-Score:  0.0\n",
      "Epoch 1/5\n",
      "  34/2245 [..............................] - ETA: 3s - loss: 0.3060 - accuracy: 0.8943 "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sabu/anaconda3/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1221: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2245/2245 [==============================] - 4s 2ms/step - loss: 0.6181 - accuracy: 0.8954\n",
      "Epoch 2/5\n",
      "2245/2245 [==============================] - 4s 2ms/step - loss: 0.5867 - accuracy: 0.8966\n",
      "Epoch 3/5\n",
      "2245/2245 [==============================] - 4s 2ms/step - loss: 0.4439 - accuracy: 0.8971\n",
      "Epoch 4/5\n",
      "2245/2245 [==============================] - 4s 2ms/step - loss: 0.4300 - accuracy: 0.8971\n",
      "Epoch 5/5\n",
      "2245/2245 [==============================] - 4s 2ms/step - loss: 0.4268 - accuracy: 0.8972\n",
      "Fold 4 Accuracy: 0.8972705751288121  Precision:  0.0  Recall:  0.0  F1-Score:  0.0\n",
      "Epoch 1/5\n",
      "2245/2245 [==============================] - 3s 2ms/step - loss: 0.4926 - accuracy: 0.8967\n",
      "Epoch 2/5\n",
      "2245/2245 [==============================] - 3s 2ms/step - loss: 0.5177 - accuracy: 0.8970\n",
      "Epoch 3/5\n",
      "2245/2245 [==============================] - 3s 2ms/step - loss: 0.3998 - accuracy: 0.8970\n",
      "Epoch 4/5\n",
      "2245/2245 [==============================] - 3s 2ms/step - loss: 0.4625 - accuracy: 0.8968\n",
      "Epoch 5/5\n",
      "2245/2245 [==============================] - 3s 2ms/step - loss: 0.3910 - accuracy: 0.8970\n",
      "Fold 5 Accuracy: 0.8972705751288121  Precision:  0.0  Recall:  0.0  F1-Score:  0.0\n"
     ]
    }
   ],
   "source": [
    "# skf = StratifiedKFold(n_splits=5)\n",
    "# fold_no = 1\n",
    "# for train_index, test_index in skf.split(X1, Y):\n",
    "#     X_train = X1[train_index,:]\n",
    "#     X_test = X1[test_index,:]\n",
    "#     Y_train = Y[train_index,:]\n",
    "#     Y_test = Y[test_index,:]\n",
    "#     model.fit(X_train, Y_train, epochs=5,batch_size=128,)\n",
    "#     predictions = model.predict(X_test)\n",
    "#     print('Fold',str(fold_no),'Accuracy:',accuracy_score(Y_test,np.round(predictions)), ' Precision: ',precision_score(Y_test,np.round(predictions)), ' Recall: ',recall_score(Y_test,np.round(predictions)), ' F1-Score: ',f1_score(Y_test,np.round(predictions)))\n",
    "#     fold_no += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2-gram vectorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bigram_vector = CountVectorizer(ngram_range=(2, 2),token_pattern=r'\\b\\w+\\b', min_df=1)\n",
    "# X_ngram = bigram_vector.fit_transform(data1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# svd2 = TruncatedSVD(n_components = 100)\n",
    "# X_ngram = svd2.fit_transform(X_ngram)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X2 = np.hstack((X_ngram,all_features))\n",
    "# X2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# models2 = tf.keras.Sequential([\n",
    "\n",
    "    \n",
    "#     layers.Flatten(input_dim = len(X1[0,:])), \n",
    "    \n",
    "#     layers.Dense(units=256, activation=None), \n",
    "#     layers.BatchNormalization(),\n",
    "#     layers.Activation(activations.relu),\n",
    "    \n",
    "#     layers.Dense(units=128, activation=None), \n",
    "#     layers.BatchNormalization(),\n",
    "#     layers.Activation(activations.relu),\n",
    "    \n",
    "#     layers.Dense(units=1, activation=activations.sigmoid) \n",
    "# ])\n",
    "\n",
    "# models2.summary()\n",
    "\n",
    "# models2.compile(\n",
    "#     optimizer=optimizers.Adam(learning_rate=0.001), \n",
    "#     loss=losses.BinaryCrossentropy(), \n",
    "#     metrics=['accuracy']\n",
    "# ) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# skf = StratifiedKFold(n_splits=5)\n",
    "# fold_no = 1\n",
    "# for train_index, test_index in skf.split(X2, Y):\n",
    "#     X_train = X2[train_index,:]\n",
    "#     X_test = X2[test_index,:]\n",
    "#     Y_train = Y[train_index,:]\n",
    "#     Y_test = Y[test_index,:]\n",
    "#     history1 = models1.fit(X_train, Y_train, epochs=5,batch_size=128,)\n",
    "#     predictions = models1.predict(X_test)\n",
    "#     print('Fold',str(fold_no),'Accuracy:',accuracy_score(Y_test,np.round(predictions)), ' Precision: ',precision_score(Y_test,np.round(predictions)), ' Recall: ',recall_score(Y_test,np.round(predictions)), ' F1-Score: ',f1_score(Y_test,np.round(predictions)))\n",
    "#     fold_no += 1=0.1,\n",
    "# ) "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
